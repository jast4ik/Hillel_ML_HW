{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed = 42\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, BatchNormalization, Input, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from keras_tuner import HyperParameters, Hyperband\n",
    "\n",
    "from os.path import isfile\n",
    "\n",
    "import lzma\n",
    "\n",
    "import pickle\n",
    "\n",
    "from jax import vmap, jit\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Импорт данных**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Визуализация исходных данных**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 5, figsize=(10, 10))\n",
    "axs = ax.flatten()\n",
    "\n",
    "letters_idx = np.random.randint(0, X_test.shape[0], len(axs))\n",
    "\n",
    "for idx in range(len(axs)):\n",
    "    axs[idx].imshow(X_test[letters_idx[idx]], cmap='gray_r')\n",
    "    axs[idx].set_title(y_test[letters_idx[idx]])\n",
    "    axs[idx].xaxis.set_ticks([])\n",
    "    axs[idx].yaxis.set_ticks([])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height, depth = X_train.shape[1], X_train.shape[2], 1\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], width, height, depth)\n",
    "X_train = X_train.astype(np.float32)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], width, height, depth)\n",
    "X_test = X_test.astype(np.float32)\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Аугментация**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мне показалось скучным работать с исходными данными - слишком они хороши. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_SHIFT = [-1, 1]\n",
    "H_SHIFT = [-1, 1]\n",
    "ROT_ANGLE = 20\n",
    "ZOOM_RANGE = [0.7, 1.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "P_WIDTH = 28\n",
    "P_HEIGHT = 28\n",
    "P_DEPTH = 1\n",
    "N_CLASSES = 10\n",
    "NUM_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я много раз читал о создании валидационного датасета, который является частью тренировочного. Лучше один раз сделать, чем сто раз прочитать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=True, stratify=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В принципе, генератор можно настроить на разделение датасета. Чтобы не делать несколько отдельных. Но мне показалось так интереснее. Серьезного обоснования такому решению нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = ImageDataGenerator(\n",
    "    width_shift_range=W_SHIFT, \n",
    "    height_shift_range=H_SHIFT,\n",
    "    rotation_range=ROT_ANGLE,\n",
    "    zoom_range=ZOOM_RANGE,\n",
    "    samplewise_center=True,\n",
    "    samplewise_std_normalization=True,\n",
    "    rescale= 1.0/255.0\n",
    ")\n",
    "\n",
    "train_generator.fit(X_train)\n",
    "train_iterator = train_generator.flow(X_train, y_train, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_generator = ImageDataGenerator(\n",
    "    width_shift_range=W_SHIFT, \n",
    "    height_shift_range=H_SHIFT,\n",
    "    rotation_range=ROT_ANGLE,\n",
    "    zoom_range=ZOOM_RANGE,\n",
    "    samplewise_center=True,\n",
    "    samplewise_std_normalization=True,\n",
    "    rescale= 1.0/255.0\n",
    ")\n",
    "\n",
    "val_generator.fit(X_val)\n",
    "val_iterator = val_generator.flow(X_val, y_val, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = ImageDataGenerator(\n",
    "    samplewise_center=True,\n",
    "    samplewise_std_normalization=True,\n",
    "    rescale=1.0/255.0\n",
    ")\n",
    "\n",
    "test_generator.fit(X_test)\n",
    "test_iterator = test_generator.flow(X_test, y_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Визуализация аугментированных данных**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 5, figsize=(10, 10))\n",
    "axs = ax.flatten()\n",
    "\n",
    "for idx in range(len(axs)):\n",
    "    letter_idx = round(letters_idx[idx]/BATCH_SIZE)\n",
    "    random_bath_idx = random.choice(range(BATCH_SIZE))\n",
    "    \n",
    "    axs[idx].imshow(train_iterator[letter_idx][0][random_bath_idx], cmap='gray_r')\n",
    "    \n",
    "    img_label = np.nonzero(train_iterator[letter_idx][1][random_bath_idx])\n",
    "    \n",
    "    axs[idx].set_title(int(img_label[0]))\n",
    "    axs[idx].xaxis.set_ticks([])\n",
    "    axs[idx].yaxis.set_ticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Модель**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У меня нет серьезного обоснования, почему я взял именно такую структуру. Это некоторая комбинация изученных примеров и моего понимания свёрточных сетей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_keras_model(hp):\n",
    "    \n",
    "    hp_filters_l1 = hp.Int(name='filters L1', min_value=16, max_value=128, step=16, default=32)\n",
    "    hp_filters_l2 = hp.Int(name='filters L2', min_value=16, max_value=128, step=16, default=64)\n",
    "    hp_weights = hp.Choice(name='kernel_initializer', values=['he_uniform', 'he_normal', 'normal', 'uniform', 'glorot_uniform', 'glorot_normal'], default='he_uniform')\n",
    "    hp_funcs = hp.Choice(name='activation', values=['relu', 'sigmoid', 'tanh'], default='relu')\n",
    "    hp_units = hp.Int(name='units', min_value=64, max_value=2048, step=64, default=128) \n",
    "    hp_rates = hp.Float(name='rate', min_value=0.0, max_value=0.5, step=0.05, default=0.25)\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4], default=1e-3)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Input((width, height, depth)))\n",
    "\n",
    "    model.add(Conv2D(filters=hp_filters_l1, kernel_size=(3, 3), activation=hp_funcs, kernel_initializer=hp_weights, padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    #model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(filters=hp_filters_l2, kernel_size=(3, 3), activation=hp_funcs, kernel_initializer=hp_weights, padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    #model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(units=hp_units, activation=hp_funcs, kernel_initializer=hp_weights))\n",
    "    model.add(Dropout(rate=hp_rates))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_from_iterator(steps_count = 0, jax = True, iterator = None):\n",
    "    \"\"\"Generates dataset from ImageDataGenerator\"\"\"\n",
    "\n",
    "    iterator.reset()\n",
    "\n",
    "    result = [[], []]\n",
    "\n",
    "    if jax:\n",
    "        result = {\n",
    "            'image': [], \n",
    "            'label': []\n",
    "        }\n",
    "\n",
    "    for _ in range(steps_count):\n",
    "        batch_ds = tf.data.Dataset.from_generator(\n",
    "            lambda: iterator,\n",
    "            output_types=(tf.float32, tf.float32),\n",
    "            output_shapes=(\n",
    "                [BATCH_SIZE, P_WIDTH, P_HEIGHT, P_DEPTH],\n",
    "                [BATCH_SIZE, N_CLASSES]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        temp_ds_iterator = iter(batch_ds)\n",
    "        batch = next(temp_ds_iterator)\n",
    "\n",
    "        if jax:\n",
    "            result['image'].append(tfds.as_numpy(batch[0]))\n",
    "            result['label'].append(np.argmax(tfds.as_numpy(batch[1]), axis=1))\n",
    "        else:\n",
    "            result[0].append(np.float32(tfds.as_numpy(batch[0])))\n",
    "            result[1].append(np.float32(tfds.as_numpy(batch[1])))\n",
    "    \n",
    "    if jax:\n",
    "        result['image'] = jnp.float32(np.array(result['image']).reshape(steps_count * BATCH_SIZE, P_WIDTH, P_HEIGHT, P_DEPTH))\n",
    "        result['label'] = jnp.float32(np.array(result['label']).flatten())\n",
    "        return result\n",
    "    else:\n",
    "        return np.array(result[0]).reshape(steps_count * BATCH_SIZE, P_WIDTH, P_HEIGHT, P_DEPTH), np.array(result[1]).reshape(steps_count * BATCH_SIZE, N_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE_TRAIN = train_iterator.n // train_iterator.batch_size\n",
    "STEP_SIZE_VAL = val_iterator.n // val_iterator.batch_size\n",
    "STEP_SIZE_TEST = test_iterator.n // test_iterator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ds, y_train_ds = make_dataset_from_iterator(STEP_SIZE_TRAIN, False, train_iterator)\n",
    "X_val_ds, y_val_ds = make_dataset_from_iterator(STEP_SIZE_VAL, False, val_iterator)\n",
    "X_test_ds, y_test_ds = make_dataset_from_iterator(STEP_SIZE_TEST, False, test_iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_iterator.reset()\n",
    "# val_iterator.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, mode='min', verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала натренирую сетку с дефолтными параметрами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = None\n",
    "keras_history = None\n",
    "\n",
    "if isfile(\"./models/keras_model.hdf5\"):\n",
    "    keras_model = load_model(\"./models/keras_model.hdf5\")\n",
    "    \n",
    "    with lzma.open(\"./models/keras_history.xz\", \"rb\") as m_file:\n",
    "        keras_history = pickle.load(m_file)\n",
    "else:\n",
    "    keras_model = build_keras_model(HyperParameters())\n",
    "    keras_history = keras_model.fit(\n",
    "        x=X_train_ds, y=y_train_ds,#train_iterator, \n",
    "        epochs=NUM_EPOCHS, \n",
    "        steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "        validation_data=(X_val_ds, y_val_ds),#val_iterator,\n",
    "        validation_steps=STEP_SIZE_VAL, \n",
    "        callbacks=[keras_early_stop]\n",
    "        )\n",
    "  \n",
    "    keras_model.save(\"./models/keras_model.hdf5\")\n",
    "\n",
    "    with lzma.open(\"./models/keras_history.xz\", \"wb\") as m_file:\n",
    "        pickle.dump(keras_history.history, m_file)\n",
    "\n",
    "    keras_history = keras_history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И теперь попробую подобрать гиперпараметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras_tuner = Hyperband(\n",
    "#     hypermodel=build_keras_model,\n",
    "#     objective='val_accuracy',\n",
    "#     factor=3,\n",
    "#     max_epochs=10,\n",
    "#     directory='./tf_data', \n",
    "#     project_name='HW14'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras_tuner_early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1, mode='min', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_iterator.reset()\n",
    "# val_iterator.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras_tuner.search(\n",
    "#     train_iterator,\n",
    "#     validation_data=val_iterator,\n",
    "#     callbacks=[keras_tuner_early_stop]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_tuned_parameters = keras_tuner.get_best_hyperparameters()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_iterator.reset()\n",
    "# val_iterator.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_tuned_model = None\n",
    "# best_tuned_history = None\n",
    "\n",
    "# if isfile(\"./models/keras_tuned_model.hdf5\"):\n",
    "#     best_tuned_model = load_model(\"./models/keras_tuned_model.hdf5\")\n",
    "    \n",
    "#     with lzma.open(\"./models/keras_tuned_history.xz\", \"rb\") as m_file:\n",
    "#         best_tuned_history = pickle.load(m_file)\n",
    "# else:\n",
    "#     best_tuned_model = build_keras_model(best_tuned_parameters)\n",
    "#     best_tuned_history = best_tuned_model.fit(\n",
    "#         train_iterator, \n",
    "#         epochs=10, \n",
    "#         steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "#         validation_data=val_iterator,\n",
    "#         validation_steps=STEP_SIZE_VAL, \n",
    "#         callbacks=[keras_early_stop]\n",
    "#         )\n",
    "  \n",
    "#     best_tuned_model.save(\"./models/keras_tuned_model.hdf5\")\n",
    "\n",
    "#     with lzma.open(\"./models/keras_tuned_history.xz\", \"wb\") as m_file:\n",
    "#         pickle.dump(best_tuned_history.history, m_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(hist: dict):\n",
    "    fig, ax = plt.subplots(2,1, figsize=(18, 10))\n",
    "\n",
    "    ax[0].plot(hist['loss'], color='b', label=\"Training loss\")\n",
    "    ax[0].plot(hist['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n",
    "    legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "    ax[1].plot(hist['accuracy'], color='b', label=\"Training accuracy\")\n",
    "    ax[1].plot(hist['val_accuracy'], color='r',label=\"Validation accuracy\")\n",
    "    legend = ax[1].legend(loc='best', shadow=True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_history(best_tuned_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(keras_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(in_model: None, iterator: None, print_result=False):\n",
    "    step_size = iterator.n // iterator.batch_size\n",
    "    verb = -1\n",
    "\n",
    "    if print_result:\n",
    "        verb = 1\n",
    "\n",
    "    iterator.reset()\n",
    "\n",
    "    ls, acc = in_model.evaluate(iterator, steps=step_size, verbose=verb)\n",
    "\n",
    "    if print_result:\n",
    "        print(\"Loss:\\t\\t{:.6f}\\nAccuracy:\\t{:.6f}\".format(ls, acc))\n",
    "    else:\n",
    "        return ls, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_model(best_tuned_model, test_iterator, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(keras_model, test_iterator, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тренированая модель показала худшую точность, что не радует. Но обучилась на две эпохи быстрее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Реальные цифры**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_img_generator = ImageDataGenerator(\n",
    "    samplewise_center=True,\n",
    "    samplewise_std_normalization=True,\n",
    "    rescale=1.0/255.0\n",
    ")\n",
    "\n",
    "#test_generator.fit(X_test)\n",
    "true_img_iterator = true_img_generator.flow_from_directory(\n",
    "    directory=\"./data\",\n",
    "    target_size=(28, 28),\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=1,\n",
    "    class_mode=\"categorical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_img_iterator.reset()\n",
    "\n",
    "fig, ax = plt.subplots(5, 2, figsize=(5, 10))\n",
    "axs = ax.flatten()\n",
    "\n",
    "for idx in range(len(axs)):\n",
    "    axs[idx].imshow(np.reshape(true_img_iterator[idx][0], (28, 28, 1)), cmap='gray_r')\n",
    "    #axs[idx].imshow(true_img_iterator[idx][0])\n",
    "    \n",
    "    img_label = np.nonzero(true_img_iterator[idx][1][0])\n",
    "    \n",
    "    axs[idx].set_title(int(img_label[0]))\n",
    "\n",
    "    axs[idx].xaxis.set_ticks([])\n",
    "    axs[idx].yaxis.set_ticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут интересно, что фон не такой чистый и вобще цифры немного отличаются по масштабу и повороту. Отлично, это возможность проверить влияние аугментации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_img_iterator.reset()\n",
    "\n",
    "# evaluate_model(best_tuned_model, true_img_iterator, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_img_iterator.reset()\n",
    "\n",
    "evaluate_model(keras_model, true_img_iterator, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss на тренированной модели меньше. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_img_iterator.reset()\n",
    "\n",
    "# fig, ax = plt.subplots(5, 2, figsize=(5, 10))\n",
    "# axs = ax.flatten()\n",
    "\n",
    "# for idx in range(len(true_img_iterator)):\n",
    "#     best_model_predict = best_tuned_model.predict(true_img_iterator[idx][0]).argmax(-1)\n",
    "#     keras_model_predict = keras_model.predict(true_img_iterator[idx][0]).argmax(-1)\n",
    "#     ground_true = np.nonzero(true_img_iterator[idx][1])[1]\n",
    "\n",
    "#     axs[idx].imshow(np.reshape(true_img_iterator[idx][0], (28, 28, 1)), cmap='gray_r')\n",
    "    \n",
    "#     axs[idx].set_title(\"True: {}\\nBest trained: {}\\n Keras {}\".format(ground_true, best_model_predict, keras_model_predict))\n",
    "\n",
    "#     axs[idx].xaxis.set_ticks([])\n",
    "#     axs[idx].yaxis.set_ticks([])\n",
    "\n",
    "# plt.tight_layout(pad=2.0)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, семерка вобще не хочет определяться. Можно бы было перетренировать модели, но я не вижу в этом смысла. Предварительно могу сказать, это связанно с тем, что в датасете почти все семерки без горизонтальной линии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Заключение**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чем сетка проще, тем лучше. Inference быстрее, обучение быстрее. Тот случай, когда кашу маслом легко испортить. Как показал эксперимент точность может упасть вместе с производительностью, что печально."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подбор гиперпараметров тут тоже не панацея. Сетки нужно понимать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аугментация данных важный момент. Для картинок точно. Сетка без аугментации половину реальных цифр не узнала (этот эксперимент я не сохранил). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важность ранней остановки трудно недооценить. Это не только время, но и переобучение. Можно оставить сетку учится и не заметить, где она в оверфит ушла. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вобще callbacks это must have технология для применения в нейронках."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow порадовал сохранением данных самостоятельно. Можно прервать обучение и не проводить его заново с нуля - очень удобно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большой (возможно единственный), конечно, минус нейронок это скорость. Что обучения, что inference. Но, безусловно, нелинейность заложенная изначально в саму идею творит чудеса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **P.S. Flax**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Никакой моей заслуги в приведенном ниже коде нет, это совсем немного переделанный тутор <a href = https://flax.readthedocs.io/en/latest/notebooks/annotated_mnist.html>Annotated MNIST</a>.<br>\n",
    "Просто мне стало интересно сравнить производительность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = make_dataset_from_iterator(STEP_SIZE_TRAIN, True, train_iterator)\n",
    "val_ds = make_dataset_from_iterator(STEP_SIZE_VAL, True, val_iterator)\n",
    "test_ds = make_dataset_from_iterator(STEP_SIZE_TEST, True, test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DS_SIZE = len(train_ds['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x_in):\n",
    "        x = nn.Conv(features=32, kernel_size=(3, 3), padding='SAME')(x_in)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = nn.Conv(features=64, kernel_size=(3, 3), padding='SAME')(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = x.reshape((x.shape[0], -1))\n",
    "        x = nn.Dense(features=128)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=N_CLASSES)(x)\n",
    "        x = nn.log_softmax(x)\n",
    "        return x\n",
    "\n",
    "def cross_entropy_loss(*, logits, labels):\n",
    "    one_hot_labels = jax.nn.one_hot(labels, num_classes=N_CLASSES)\n",
    "    return -jnp.mean(jnp.sum(one_hot_labels * logits, axis=-1))\n",
    "\n",
    "def compute_metrics(*, logits, labels):\n",
    "    loss = cross_entropy_loss(logits=logits, labels=labels)\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "    metrics = {\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy,\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def create_train_state(rng, learning_rate, momentum):\n",
    "    cnn = CNN()\n",
    "    params = cnn.init(rng, jnp.ones([1, P_WIDTH, P_HEIGHT, P_DEPTH]))['params']\n",
    "    tx = optax.sgd(learning_rate, momentum)\n",
    "    \n",
    "    return train_state.TrainState.create(apply_fn=cnn.apply, params=params, tx=tx)\n",
    "\n",
    "@jit\n",
    "def train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        logits = CNN().apply({'params': params}, batch['image'])\n",
    "        loss = cross_entropy_loss(logits=logits, labels=batch['label'])\n",
    "        \n",
    "        return loss, logits\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (_, logits), grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics = compute_metrics(logits=logits, labels=batch['label'])\n",
    "    \n",
    "    return state, metrics\n",
    "\n",
    "@jit\n",
    "def eval_step(params, batch):\n",
    "    logits = CNN().apply({'params': params}, batch['image'])\n",
    "    return compute_metrics(logits=logits, labels=batch['label'])\n",
    "\n",
    "\n",
    "def train_epoch(state, train_ds, batch_size, epoch, rng):\n",
    "    #train_ds_size = len(train_ds['image'])\n",
    "    steps_per_epoch = STEP_SIZE_TRAIN#train_ds_size // batch_size\n",
    "\n",
    "    perms = jax.random.permutation(rng, TRAIN_DS_SIZE)\n",
    "    perms = perms[:steps_per_epoch * batch_size]  # skip incomplete batch\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "    batch_metrics = []\n",
    "    \n",
    "    for perm in perms:\n",
    "        batch = {k: v[perm, ...] for k, v in train_ds.items()}\n",
    "        state, metrics = train_step(state, batch)\n",
    "        batch_metrics.append(metrics)\n",
    "\n",
    "    batch_metrics_np = jax.device_get(batch_metrics)\n",
    "    epoch_metrics_np = {\n",
    "        k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
    "        for k in batch_metrics_np[0]\n",
    "        }\n",
    "\n",
    "    #print('train epoch: %d, loss: %.4f, accuracy: %.2f' % (epoch, epoch_metrics_np['loss'], ))\n",
    "    train_metrics = [epoch_metrics_np['loss'], epoch_metrics_np['accuracy'] * 100]\n",
    "\n",
    "    return train_metrics, state\n",
    "\n",
    "def eval_model(params, test_ds):\n",
    "    metrics = eval_step(params, test_ds)\n",
    "    metrics = jax.device_get(metrics)\n",
    "    summary = jax.tree_map(lambda x: x.item(), metrics)\n",
    "    return summary['loss'], summary['accuracy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "\n",
    "learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "\n",
    "state = create_train_state(init_rng, learning_rate, momentum)\n",
    "del init_rng  # Must not be used anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    start_time = time.time()\n",
    "    rng, input_rng = jax.random.split(rng)\n",
    "\n",
    "    train_metrics, state = train_epoch(state, train_ds, BATCH_SIZE, epoch, input_rng)\n",
    "    test_loss, test_accuracy = eval_model(state.params, val_ds)\n",
    "\n",
    "    #print(' test epoch: %d, loss: %.2f, accuracy: %.2f' % (epoch, test_loss, test_accuracy * 100))\n",
    "    print(\"Epoch #{}: {:.1f}s\\ttrain loss: {:.5f}\\t train acc: {:.5f}\\ttest loss: {:.5f}\\t test acc: {:.5f}\".format(\n",
    "      str(epoch), (time.time() - start_time), train_metrics[0], train_metrics[1], test_loss, test_accuracy\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
