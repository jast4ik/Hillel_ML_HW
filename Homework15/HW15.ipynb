{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import dataframe as ddf\n",
    "from dask.distributed import Client, wait, LocalCluster\n",
    "from dask import delayed, compute\n",
    "from dask_ml.wrappers import ParallelPostFit\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gc\n",
    "import ctypes\n",
    "\n",
    "from os.path import isfile\n",
    "import lzma\n",
    "import pickle\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Normalization, Dense, LSTM, Embedding, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Порядок исследования**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изучая LSTM, я узнал о том, что на смену LSTM уже пришли трансформеры, а на смену им Attention.<br>\n",
    "Поэтому в этой работе я решил не столько изучить LSTM (а точнее рекурентные сети как таковые), сколько с ими познакомиться и покрутить в руках NLP.<br>\n",
    "Но поскольку перепостить чужой код скучно, я нашел себе приключение.\n",
    "1. Познакомиться, наконец, с Dask.\n",
    "2. Проверить результаты работы <a href=\"https://arxiv.org/abs/1903.07288#:~:text=Since%20LSTMs%20and%20CNNs%20take,comes%20to%20performance%20and%20accuracies.\">EFFECTS OF PADDING ON LSTMS AND CNNS</a><br>\n",
    "В работе утверждается, что есть существенное влияние на точность модели от применения pre или post padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я буду, само-собой, использовать локальный кластер. Хотя нет особой проблемы подключиться к тому же GCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/doska/Hillel_ML_HW/Homework15/dask-worker-space/worker-r1rtddc8', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/doska/Hillel_ML_HW/Homework15/dask-worker-space/worker-gmwfbcqz', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/doska/Hillel_ML_HW/Homework15/dask-worker-space/worker-dzqdixoa', purging\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home/doska/Hillel_ML_HW/Homework15/dask-worker-space/worker-uxqft4by', purging\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a443b6b06842afa640e19767511393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(HTML(value='<div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-outpu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster = LocalCluster(memory_limit='24GB')\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не знаю, свойство ли это локального кластера исключительно, но на nix системах имеется баг - не освобождается вовремя память. <br>\n",
    "Функция ниже взята из официальной документации как раз для устранения этой проблемы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_memory() -> int:\n",
    "     libc = ctypes.CDLL(\"libc.so.6\")\n",
    "     return libc.malloc_trim(0)\n",
    "\n",
    "def clear_memory():\n",
    "     temp = client.run(trim_memory)\n",
    "     temp = client.run(gc.collect)\n",
    "     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/doska/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/doska/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/doska/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это глупая строчка, на самом деле. Какой смысл использовать Dask там где датасет помещается в память? Никакого, но я просто пробую инструмент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_df = ddf.read_csv(urlpath='./data/trip-advisor-hotel-reviews.zip', compression='zip', blocksize=None).repartition(npartitions=32)\n",
    "src_df = client.persist(src_df)\n",
    "wait_result = wait(src_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rating</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Review\n",
       "Rating        \n",
       "1         1421\n",
       "2         1793\n",
       "3         2184\n",
       "4         6039\n",
       "5         9054"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_df.groupby(by='Rating').count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proceed_string(text=None):\n",
    "    step_1 = re.sub(\"<.*?>\", \" \", text)\n",
    "    step_2 = step_1.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    step_3 = nltk.word_tokenize(step_2)\n",
    "    step_4 = [LancasterStemmer().stem(x) for x in step_3]\n",
    "    return \" \".join(step_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь я раскидываю выполнение функции на всех workers. Получается быстрее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = None\n",
    "\n",
    "if isfile(\"./data/X.xz\"):\n",
    "    with lzma.open(\"./data/X.xz\", \"rb\") as m_file:\n",
    "        X = pickle.load(m_file)\n",
    "else:\n",
    "    futures = client.map(proceed_string, src_df['Review'])\n",
    "    X = client.gather(futures)\n",
    "\n",
    "    del futures  \n",
    "    with lzma.open(\"./data/X.xz\", \"wb\") as m_file:\n",
    "        pickle.dump(X, m_file)\n",
    "\n",
    "\n",
    "y = src_df.compute()['Rating'].tolist()\n",
    "\n",
    "del src_df, wait_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У Dask есть замечательная возможность объявить тяжолую фичу тяжелой. То есть не загружать её целиком в память, а пользовать по кусочкам по мере надобности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = [0 if x <=3 else 1 for x in y]\n",
    "\n",
    "y_binary_mem = client.scatter(y_)\n",
    "y_multi_mem = client.scatter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def train_logistic_regression(X_in, y_in):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_in, y_in, shuffle=True, random_state=42, stratify=y_in)\n",
    "\n",
    "    clf = ParallelPostFit(estimator=LogisticRegression(max_iter=2000, random_state=42, n_jobs=1), scoring='accuracy')\n",
    "    \n",
    "    # Можно было бы сделать так. Тогда sklearn использовал бы dask parallel backend.\n",
    "\n",
    "    # import joblib\n",
    "    \n",
    "    # clf = LogisticRegression(max_iter=2000, random_state=42, n_jobs=-1)\n",
    "\n",
    "    # with joblib.parallel_backend('dask'):\n",
    "    #     clf.fit(X_train, y_train)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    return accuracy_score(y_test, clf.predict(X_test)) * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея кода ниже следующая. Самые тяжолые задачи я закидываю на кластер с самым высоким приоритетом. <br>\n",
    "Пока они будут считаться, на свободных workers будут считаться более легкие.<br>\n",
    "Вобще это интересный подход - оптимизировать очередь выполнения. И эффективный."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_processors():\n",
    "    results = []\n",
    "    ngram_values = [3, 2, 1]\n",
    "\n",
    "    X_ = client.scatter(X)\n",
    "\n",
    "    for i in ngram_values:\n",
    "        count_vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, i), lowercase=False)  \n",
    "        X_mem = client.submit(count_vectorizer.fit_transform, X_)\n",
    "        results.append(['CountVectorizer', 'binary', i, client.submit(train_logistic_regression, X_mem, y_binary_mem, priority=(i+2))])\n",
    "        results.append(['CountVectorizer', 'multi', i, client.submit(train_logistic_regression, X_mem, y_multi_mem, priority=(i+3))])\n",
    "\n",
    "        tfid_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, i), lowercase=False)\n",
    "        X_mem = client.submit(tfid_vectorizer.fit_transform, X_)\n",
    "        results.append(['TfidfVectorizer', 'binary', i, client.submit(train_logistic_regression, X_mem, y_binary_mem, priority=i)])\n",
    "        results.append(['TfidfVectorizer', 'multi', i, client.submit(train_logistic_regression, X_mem, y_multi_mem, priority=(i+1))])\n",
    "\n",
    "    mb = MultiLabelBinarizer(sparse_output=False)\n",
    "    X_mem = client.submit(mb.fit_transform, X_)\n",
    "    results.append(['MultiLabelBinarizer', 'binary', i, client.submit(train_logistic_regression, X_mem, y_binary_mem)])\n",
    "    results.append(['MultiLabelBinarizer', 'multi', i, client.submit(train_logistic_regression, X_mem, y_multi_mem)])\n",
    "\n",
    "    results = client.gather(results)\n",
    "    results = compute(results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "if isfile(\"./data/compare.xz\"):\n",
    "    with lzma.open(\"./data/compare.xz\", \"rb\") as m_file:\n",
    "        results = pickle.load(m_file)\n",
    "else:\n",
    "    results = compare_processors()\n",
    "    clear_memory()\n",
    "            \n",
    "    with lzma.open(\"./data/compare.xz\", \"wb\") as m_file:\n",
    "        pickle.dump(results, m_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask скушал все эти невменяемые таблицы (даже без sparse_output=True) и не вспотел. По крайней мере ничего не вылетело, лимит памяти не был привышен. Очень полезное свойство!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Processor</th>\n",
       "      <th>Classification</th>\n",
       "      <th>n-gram</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">CountVectorizer</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">binary</th>\n",
       "      <th>1</th>\n",
       "      <td>88.131954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89.283623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.400742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">multi</th>\n",
       "      <th>1</th>\n",
       "      <td>57.622487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60.628538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61.214132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">MultiLabelBinarizer</th>\n",
       "      <th>binary</th>\n",
       "      <th>1</th>\n",
       "      <td>73.648253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi</th>\n",
       "      <th>1</th>\n",
       "      <td>43.607261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">TfidfVectorizer</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">binary</th>\n",
       "      <th>1</th>\n",
       "      <td>89.361702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87.292602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84.149912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">multi</th>\n",
       "      <th>1</th>\n",
       "      <td>60.648058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59.047433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55.299629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Score\n",
       "Processor           Classification n-gram           \n",
       "CountVectorizer     binary         1       88.131954\n",
       "                                   2       89.283623\n",
       "                                   3       89.400742\n",
       "                    multi          1       57.622487\n",
       "                                   2       60.628538\n",
       "                                   3       61.214132\n",
       "MultiLabelBinarizer binary         1       73.648253\n",
       "                    multi          1       43.607261\n",
       "TfidfVectorizer     binary         1       89.361702\n",
       "                                   2       87.292602\n",
       "                                   3       84.149912\n",
       "                    multi          1       60.648058\n",
       "                                   2       59.047433\n",
       "                                   3       55.299629"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.DataFrame(columns=['Processor', 'Classification', 'n-gram', 'Score'])\n",
    "a['Processor'] = [x[0] for x in results]\n",
    "a['Classification'] = [x[1] for x in results]\n",
    "a['n-gram'] = [x[2] for x in results]\n",
    "a['Score'] = [x[3] for x in results]\n",
    "\n",
    "a.groupby(by=['Processor', 'Classification', 'n-gram']).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Довольно интересно, что TfidfVectorizer от использования ngrams существенно теряет в точности. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del y_binary_mem, y_multi_mem\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше будет стандартный пайплайн для LSTM. Токенизация, padding, embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    20491.000000\n",
       "mean       104.377239\n",
       "std        100.665153\n",
       "min          7.000000\n",
       "25%         48.000000\n",
       "50%         77.000000\n",
       "75%        124.000000\n",
       "max       1931.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_lengts = np.array([len(x) for x in [y.split() for y in X]])\n",
    "\n",
    "pd.Series(review_lengts).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_tokinizer = Tokenizer(lower=False)\n",
    "keras_tokinizer.fit_on_texts(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 16\n",
    "VOCAB_SIZE = len(list(keras_tokinizer.word_index)) + 1\n",
    "NUM_EPOCHS = 16\n",
    "BATCH_SIZE = 64\n",
    "PAD_LENGTH = int(np.array(review_lengts).mean())\n",
    "INITIAL_LR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nn = keras_tokinizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_binary = np.array(y_)\n",
    "y_multi = np.array(pd.get_dummies(y).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_exp_decay(epoch, lr):\n",
    "    k = 0.1\n",
    "    return INITIAL_LR * np.exp(-k * epoch)\n",
    "\n",
    "def save_callback(model_name='model'):\n",
    "\n",
    "    model_fn = './data/models/' + model_name + '-best_model.hdf5'\n",
    "\n",
    "    checkpoint = ModelCheckpoint(filepath=model_fn,\n",
    "                                 monitor='val_loss',\n",
    "                                 verbose=1,\n",
    "                                 save_best_only=True,\n",
    "                                 mode='min'\n",
    "                                 )\n",
    "    return checkpoint\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, mode='min', verbose=1)\n",
    "\n",
    "adam_optimizer = Adam(learning_rate=INITIAL_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(padding_in='pre', truncating_in='pre', model_name='model', clf_type='binary'):\n",
    "    history_fn = \"./data/models/\" + model_name + \"-history.csv\"\n",
    "    model_fn = './data/models/' + model_name + '-best_model.hdf5'\n",
    "    \n",
    "    history = {}\n",
    "    model = None\n",
    "\n",
    "    if isfile(history_fn) and isfile(model_fn):\n",
    "        history = pd.read_csv(filepath_or_buffer = history_fn).to_dict()\n",
    "        model = load_model(model_fn)\n",
    "        return model, history\n",
    "\n",
    "    X_ = pad_sequences(sequences=X_nn, maxlen=PAD_LENGTH, padding=padding_in, truncating=truncating_in)\n",
    "    \n",
    "    if clf_type == 'binary':\n",
    "        model = Sequential(\n",
    "            [\n",
    "                Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, mask_zero=True),\n",
    "                LSTM(32),\n",
    "                Dense(64, activation='relu'),\n",
    "                Dropout(0.5),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        model.compile(optimizer=adam_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        Y_ = y_binary\n",
    "        \n",
    "    elif clf_type == 'multi':\n",
    "        model = Sequential(\n",
    "            [\n",
    "                Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, mask_zero=True),\n",
    "                LSTM(32),\n",
    "                Dense(64, activation='relu'),\n",
    "                Dropout(0.5),\n",
    "                Dense(5, activation='softmax')\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        Y_ = y_multi\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "    history = model.fit(\n",
    "        x=X_, \n",
    "        y=Y_,\n",
    "        validation_split=0.2,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        callbacks=[early_stop, save_callback(model_name), LearningRateScheduler(lr_exp_decay, verbose=1)],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    pd.DataFrame.from_dict(history.history).to_csv(history_fn, index=False)\n",
    "\n",
    "    return model, history.history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = {}\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== multi_post_post ====================\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "paddings = ['pre', 'post']\n",
    "truncatings = ['pre', 'post']\n",
    "clf_types = ['binary', 'multi']\n",
    "\n",
    "for current_clf_type in clf_types:\n",
    "    for current_padding in paddings:\n",
    "        for current_truncating in truncatings:\n",
    "            current_model_name = current_clf_type + '_' + current_padding + '_' + current_truncating\n",
    "            print(\"==================== {} ====================\".format(current_model_name))\n",
    "            \n",
    "            a, b = train_model(padding_in=current_padding, truncating_in=current_truncating, model_name=current_model_name, clf_type=current_clf_type)\n",
    "\n",
    "            models[current_model_name] = a\n",
    "            histories[current_model_name] = b\n",
    "\n",
    "            clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_pre_pre:  \t0.87314\n",
      "binary_pre_post:  \t0.89217\n",
      "binary_post_pre:  \t0.88143\n",
      "binary_post_post:  \t0.87875\n",
      "multi_pre_pre:  \t0.57258\n",
      "multi_pre_post:  \t0.59210\n",
      "multi_post_pre:  \t0.56648\n",
      "multi_post_post:  \t0.60015\n"
     ]
    }
   ],
   "source": [
    "for key in histories.keys():\n",
    "    print(\"{}:  \\t{:.5f}\".format(key, max(histories[key]['val_accuracy'].values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот! Умные люди оказались правы - разница есть и ничего себе какая! <br>\n",
    "Любопытно, что для мультиклассовой и бинарной классификации результаты не однаковые. <br>\n",
    "Тут я могу вынести несколько суждений:\n",
    "\n",
    "1. Наиболее значимое в отзыве идет в начале предложения. Вероятно мы (люди) имеем свойство излагать суть в начале, а потом её разъясняем. \n",
    "2. Для определения нюансов оценки вероятно важна именно центральная часть написанного.\n",
    "\n",
    "Нужно будет глубже капнуть на эту тему у психологов - в будущем точно пригодится."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Заключение**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP очень большая и сложная тема. Не могу сказать, что я с ней познакомился в этой работе. Так, немного в замочную скважину посмотрел.<br>\n",
    "Но тем не менее, очивидным является следующее. NLP очень контексто зависим. При первом же рассмотрении становится понятным, что обрабатывать текст в разных сообществах придется сильно по-разному. Тут, понятное дело, от задачи нужно исходить. Но в голове этот момент держать нужно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask хорош. Как и всякий сложный инструмент его нужно уметь использовать, но точно его стоит изучить детально."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
